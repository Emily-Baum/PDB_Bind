{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): #Las clases deben empezar con MAYUS, la clase padre es nn.Modulo y la clase hijo es ConvNet\n",
    "    \n",
    "    def __init__(self): #defines the core architecture of the model that is, all the layers with the number of neurons at each layer. CONSTRUCTOR\n",
    "        super(ConvNet, self).__init__() \n",
    "        \n",
    "        input_size = 2048\n",
    "        hidden_size1 = 20\n",
    "        hidden_size2 = 10\n",
    "        hidden_size3 = 3\n",
    "        output_size = 1\n",
    "\n",
    "        self.fl1 = nn.Linear(input_size, hidden_size1) \n",
    "        self.fl2 = nn.Linear(hidden_size1, hidden_size2) \n",
    "        self.fl3 = nn.Linear(hidden_size2, hidden_size3) \n",
    "        self.fl4 = nn.Linear(hidden_size3, output_size) \n",
    "\n",
    "    def forward(self, x): #the forward function does a forward pass in the network. It includes all the activation functions at each layer. ACCIONES\n",
    "        out = self.fl1(x)\n",
    "        out = F.relu(out) #activation function\n",
    "        out = self.fl2(out)\n",
    "        out = F.relu(out) #activation function\n",
    "        out = self.fl3(out)\n",
    "        out = F.relu(out) #activation function\n",
    "        out = self.fl4(out)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch): #the actual backpropagation step\n",
    "    model.train()\n",
    "\n",
    "    loss_func = torch.nn.L1Loss(reduction='sum') \n",
    "    for b_i, (fp, y) in enumerate(train_dataloader):\n",
    "\n",
    "        fp, y = fp.to(device), y.to(device)\n",
    "        optim.zero_grad() #despues de la optimizacion hay que reiniciar el gradiente, esto lo hace esta funcion\n",
    "        pred_prob = model(fp.float())\n",
    "        loss = loss_func(pred_prob, y)\n",
    "        loss.backward() #calcula la perdida con respecto a la entrada, backward propagation\n",
    "        optim.step() #optimization step, update weights\n",
    "        \n",
    "        if b_i % 10 == 0: #\"Cada 10 pasos\"\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(fp), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader): #Para evaluar el desempe침o de la NN\n",
    "\n",
    "    model.eval()\n",
    "    loss = 0 #inicializaci칩n de la perdida\n",
    "    loss_func = torch.nn.L1Loss(reduction='sum') \n",
    "\n",
    "    with torch.no_grad(): #El with torch.no_grad() es para hacer operaciones SIN la gradient function\n",
    "        for fp, y in test_dataloader:\n",
    "            fp, y = fp.to(device), y.to(device) \n",
    "            pred_prob = model(fp)\n",
    "            loss += loss_func(pred_prob, y).item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            \n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f},  ({:.0f}%)\\n'.format(\n",
    "        loss, len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('size: ', df.shape)\\nprint(df)\\nprint('size fp: ', fp.shape)\\nprint('size y: ', y.shape)\\nprint('y: ', y)\\n\\n\\n# Display fp and y.\\nprint('\\nFirst iteration of data set: ', next(iter(data_set)), '\\n')\\n# Print how many items are in the data set\\nprint('Length of data set: ', len(data_set), '\\n')\\n\\nprint('type: ', type(data_set[0]))\\n\\n\\n# Print entire data set\\nprint('Entire data set: ', list(DataLoader(data_set)), '\\n')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PDB(Dataset):\n",
    "    def __init__(self, fp, y):\n",
    "        self.fp = fp\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx): #idx means index\n",
    "        fp = self.fp[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        return fp, y\n",
    "\n",
    "df = pd.read_csv('pdbind_full_fp2.csv')\n",
    "\n",
    "\n",
    "fp = torch.tensor(np.array(df[df.columns[0:-1]])).type(torch.float32)\n",
    "y = torch.tensor(np.array(df['-logKd/Ki'])).type(torch.float32)\n",
    "\n",
    "#DataSet\n",
    "data_set = PDB(fp, y) #llama la clase PDB\n",
    "\n",
    "\"\"\"\n",
    "print('size: ', df.shape)\n",
    "print(df)\n",
    "print('size fp: ', fp.shape)\n",
    "print('size y: ', y.shape)\n",
    "print('y: ', y)\n",
    "\n",
    "\n",
    "# Display fp and y.\n",
    "print('\\nFirst iteration of data set: ', next(iter(data_set)), '\\n')\n",
    "# Print how many items are in the data set\n",
    "print('Length of data set: ', len(data_set), '\\n')\n",
    "\n",
    "print('type: ', type(data_set[0]))\n",
    "\n",
    "\n",
    "# Print entire data set\n",
    "print('Entire data set: ', list(DataLoader(data_set)), '\\n')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor (idx, batch) in enumerate(data_loader):\\n\\n    # Print the 'fp' data of the batch\\n    print(idx, 'fp: ', batch, '\\n')\\n\\n    # Print the 'y' data of batch\\n    print(idx, 'y: ', batch, '\\n')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "\n",
    "#DataLoader\n",
    "data_loader = DataLoader(data_set, batch_size=batch_size,shuffle=True ) #With collate_fn then the output are tensors. el tama침o del lote se establece en 2. Esto significa que cuando recorre el conjunto de datos, DataLoader generar치 2 instancias de datos en lugar de una\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = data_set, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = data_set, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for (idx, batch) in enumerate(data_loader):\n",
    "\n",
    "    # Print the 'fp' data of the batch\n",
    "    print(idx, 'fp: ', batch, '\\n')\n",
    "\n",
    "    # Print the 'y' data of batch\n",
    "    print(idx, 'y: ', batch, '\\n')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet() #Model es el objeto que tiene la clase ConvNet\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = learning_rate) #lr = learning rate , model.parameters llama a los parametros del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/9880 (0%)]\t training loss: 48.209328\n",
      "epoch: 1 [30/9880 (0%)]\t training loss: 53.128262\n",
      "epoch: 1 [60/9880 (1%)]\t training loss: 58.650112\n",
      "epoch: 1 [90/9880 (1%)]\t training loss: 47.862804\n",
      "epoch: 1 [120/9880 (1%)]\t training loss: 72.073448\n",
      "epoch: 1 [150/9880 (2%)]\t training loss: 68.350410\n",
      "epoch: 1 [180/9880 (2%)]\t training loss: 32.612431\n",
      "epoch: 1 [210/9880 (2%)]\t training loss: 57.475136\n",
      "epoch: 1 [240/9880 (2%)]\t training loss: 21.889265\n",
      "epoch: 1 [270/9880 (3%)]\t training loss: 14.254183\n",
      "epoch: 1 [300/9880 (3%)]\t training loss: 22.475275\n",
      "epoch: 1 [330/9880 (3%)]\t training loss: 18.353064\n",
      "epoch: 1 [360/9880 (4%)]\t training loss: 22.503063\n",
      "epoch: 1 [390/9880 (4%)]\t training loss: 16.614832\n",
      "epoch: 1 [420/9880 (4%)]\t training loss: 16.684958\n",
      "epoch: 1 [450/9880 (5%)]\t training loss: 13.738563\n",
      "epoch: 1 [480/9880 (5%)]\t training loss: 25.170469\n",
      "epoch: 1 [510/9880 (5%)]\t training loss: 16.491255\n",
      "epoch: 1 [540/9880 (5%)]\t training loss: 10.757460\n",
      "epoch: 1 [570/9880 (6%)]\t training loss: 10.010120\n",
      "epoch: 1 [600/9880 (6%)]\t training loss: 22.438103\n",
      "epoch: 1 [630/9880 (6%)]\t training loss: 18.946651\n",
      "epoch: 1 [660/9880 (7%)]\t training loss: 8.625010\n",
      "epoch: 1 [690/9880 (7%)]\t training loss: 9.379768\n",
      "epoch: 1 [720/9880 (7%)]\t training loss: 9.487688\n",
      "epoch: 1 [750/9880 (8%)]\t training loss: 17.259991\n",
      "epoch: 1 [780/9880 (8%)]\t training loss: 25.038105\n",
      "epoch: 1 [810/9880 (8%)]\t training loss: 9.478401\n",
      "epoch: 1 [840/9880 (9%)]\t training loss: 19.253788\n",
      "epoch: 1 [870/9880 (9%)]\t training loss: 12.196777\n",
      "epoch: 1 [900/9880 (9%)]\t training loss: 16.355787\n",
      "epoch: 1 [930/9880 (9%)]\t training loss: 23.229864\n",
      "epoch: 1 [960/9880 (10%)]\t training loss: 9.327223\n",
      "epoch: 1 [990/9880 (10%)]\t training loss: 15.162927\n",
      "epoch: 1 [1020/9880 (10%)]\t training loss: 12.855167\n",
      "epoch: 1 [1050/9880 (11%)]\t training loss: 14.835636\n",
      "epoch: 1 [1080/9880 (11%)]\t training loss: 10.359575\n",
      "epoch: 1 [1110/9880 (11%)]\t training loss: 11.466317\n",
      "epoch: 1 [1140/9880 (12%)]\t training loss: 13.731028\n",
      "epoch: 1 [1170/9880 (12%)]\t training loss: 17.542088\n",
      "epoch: 1 [1200/9880 (12%)]\t training loss: 15.696033\n",
      "epoch: 1 [1230/9880 (12%)]\t training loss: 21.366148\n",
      "epoch: 1 [1260/9880 (13%)]\t training loss: 12.187021\n",
      "epoch: 1 [1290/9880 (13%)]\t training loss: 21.893135\n",
      "epoch: 1 [1320/9880 (13%)]\t training loss: 12.260426\n",
      "epoch: 1 [1350/9880 (14%)]\t training loss: 17.176186\n",
      "epoch: 1 [1380/9880 (14%)]\t training loss: 5.561285\n",
      "epoch: 1 [1410/9880 (14%)]\t training loss: 15.047249\n",
      "epoch: 1 [1440/9880 (15%)]\t training loss: 14.195515\n",
      "epoch: 1 [1470/9880 (15%)]\t training loss: 11.657536\n",
      "epoch: 1 [1500/9880 (15%)]\t training loss: 21.555344\n",
      "epoch: 1 [1530/9880 (15%)]\t training loss: 15.205175\n",
      "epoch: 1 [1560/9880 (16%)]\t training loss: 20.265968\n",
      "epoch: 1 [1590/9880 (16%)]\t training loss: 23.090773\n",
      "epoch: 1 [1620/9880 (16%)]\t training loss: 15.841267\n",
      "epoch: 1 [1650/9880 (17%)]\t training loss: 9.678247\n",
      "epoch: 1 [1680/9880 (17%)]\t training loss: 10.645625\n",
      "epoch: 1 [1710/9880 (17%)]\t training loss: 14.843730\n",
      "epoch: 1 [1740/9880 (18%)]\t training loss: 16.633492\n",
      "epoch: 1 [1770/9880 (18%)]\t training loss: 18.119520\n",
      "epoch: 1 [1800/9880 (18%)]\t training loss: 24.320971\n",
      "epoch: 1 [1830/9880 (19%)]\t training loss: 11.892490\n",
      "epoch: 1 [1860/9880 (19%)]\t training loss: 15.435015\n",
      "epoch: 1 [1890/9880 (19%)]\t training loss: 8.907707\n",
      "epoch: 1 [1920/9880 (19%)]\t training loss: 12.402798\n",
      "epoch: 1 [1950/9880 (20%)]\t training loss: 15.343784\n",
      "epoch: 1 [1980/9880 (20%)]\t training loss: 16.035091\n",
      "epoch: 1 [2010/9880 (20%)]\t training loss: 16.845068\n",
      "epoch: 1 [2040/9880 (21%)]\t training loss: 14.422060\n",
      "epoch: 1 [2070/9880 (21%)]\t training loss: 16.054398\n",
      "epoch: 1 [2100/9880 (21%)]\t training loss: 15.858193\n",
      "epoch: 1 [2130/9880 (22%)]\t training loss: 17.405003\n",
      "epoch: 1 [2160/9880 (22%)]\t training loss: 12.037418\n",
      "epoch: 1 [2190/9880 (22%)]\t training loss: 16.465960\n",
      "epoch: 1 [2220/9880 (22%)]\t training loss: 22.556252\n",
      "epoch: 1 [2250/9880 (23%)]\t training loss: 16.438978\n",
      "epoch: 1 [2280/9880 (23%)]\t training loss: 12.732437\n",
      "epoch: 1 [2310/9880 (23%)]\t training loss: 21.370295\n",
      "epoch: 1 [2340/9880 (24%)]\t training loss: 21.930542\n",
      "epoch: 1 [2370/9880 (24%)]\t training loss: 14.214134\n",
      "epoch: 1 [2400/9880 (24%)]\t training loss: 15.514475\n",
      "epoch: 1 [2430/9880 (25%)]\t training loss: 10.156843\n",
      "epoch: 1 [2460/9880 (25%)]\t training loss: 9.628580\n",
      "epoch: 1 [2490/9880 (25%)]\t training loss: 9.234007\n",
      "epoch: 1 [2520/9880 (26%)]\t training loss: 12.037477\n",
      "epoch: 1 [2550/9880 (26%)]\t training loss: 6.290185\n",
      "epoch: 1 [2580/9880 (26%)]\t training loss: 13.892193\n",
      "epoch: 1 [2610/9880 (26%)]\t training loss: 14.273710\n",
      "epoch: 1 [2640/9880 (27%)]\t training loss: 13.855135\n",
      "epoch: 1 [2670/9880 (27%)]\t training loss: 11.838165\n",
      "epoch: 1 [2700/9880 (27%)]\t training loss: 15.146431\n",
      "epoch: 1 [2730/9880 (28%)]\t training loss: 12.237310\n",
      "epoch: 1 [2760/9880 (28%)]\t training loss: 14.337755\n",
      "epoch: 1 [2790/9880 (28%)]\t training loss: 15.301656\n",
      "epoch: 1 [2820/9880 (29%)]\t training loss: 13.618484\n",
      "epoch: 1 [2850/9880 (29%)]\t training loss: 10.016478\n",
      "epoch: 1 [2880/9880 (29%)]\t training loss: 18.893047\n",
      "epoch: 1 [2910/9880 (29%)]\t training loss: 11.876550\n",
      "epoch: 1 [2940/9880 (30%)]\t training loss: 6.553345\n",
      "epoch: 1 [2970/9880 (30%)]\t training loss: 6.586840\n",
      "epoch: 1 [3000/9880 (30%)]\t training loss: 15.647928\n",
      "epoch: 1 [3030/9880 (31%)]\t training loss: 20.161482\n",
      "epoch: 1 [3060/9880 (31%)]\t training loss: 18.440781\n",
      "epoch: 1 [3090/9880 (31%)]\t training loss: 13.556086\n",
      "epoch: 1 [3120/9880 (32%)]\t training loss: 7.193010\n",
      "epoch: 1 [3150/9880 (32%)]\t training loss: 16.288776\n",
      "epoch: 1 [3180/9880 (32%)]\t training loss: 6.499480\n",
      "epoch: 1 [3210/9880 (32%)]\t training loss: 15.367763\n",
      "epoch: 1 [3240/9880 (33%)]\t training loss: 21.745888\n",
      "epoch: 1 [3270/9880 (33%)]\t training loss: 10.697845\n",
      "epoch: 1 [3300/9880 (33%)]\t training loss: 14.149462\n",
      "epoch: 1 [3330/9880 (34%)]\t training loss: 14.139633\n",
      "epoch: 1 [3360/9880 (34%)]\t training loss: 8.945910\n",
      "epoch: 1 [3390/9880 (34%)]\t training loss: 3.767446\n",
      "epoch: 1 [3420/9880 (35%)]\t training loss: 1.574984\n",
      "epoch: 1 [3450/9880 (35%)]\t training loss: 13.625218\n",
      "epoch: 1 [3480/9880 (35%)]\t training loss: 5.020772\n",
      "epoch: 1 [3510/9880 (36%)]\t training loss: 14.950390\n",
      "epoch: 1 [3540/9880 (36%)]\t training loss: 14.404995\n",
      "epoch: 1 [3570/9880 (36%)]\t training loss: 17.062826\n",
      "epoch: 1 [3600/9880 (36%)]\t training loss: 12.640203\n",
      "epoch: 1 [3630/9880 (37%)]\t training loss: 11.143162\n",
      "epoch: 1 [3660/9880 (37%)]\t training loss: 20.868679\n",
      "epoch: 1 [3690/9880 (37%)]\t training loss: 5.106367\n",
      "epoch: 1 [3720/9880 (38%)]\t training loss: 7.814051\n",
      "epoch: 1 [3750/9880 (38%)]\t training loss: 10.001680\n",
      "epoch: 1 [3780/9880 (38%)]\t training loss: 3.562762\n",
      "epoch: 1 [3810/9880 (39%)]\t training loss: 10.588947\n",
      "epoch: 1 [3840/9880 (39%)]\t training loss: 14.666647\n",
      "epoch: 1 [3870/9880 (39%)]\t training loss: 9.894169\n",
      "epoch: 1 [3900/9880 (39%)]\t training loss: 27.407837\n",
      "epoch: 1 [3930/9880 (40%)]\t training loss: 7.142099\n",
      "epoch: 1 [3960/9880 (40%)]\t training loss: 8.489323\n",
      "epoch: 1 [3990/9880 (40%)]\t training loss: 25.433994\n",
      "epoch: 1 [4020/9880 (41%)]\t training loss: 12.243202\n",
      "epoch: 1 [4050/9880 (41%)]\t training loss: 14.945728\n",
      "epoch: 1 [4080/9880 (41%)]\t training loss: 12.629562\n",
      "epoch: 1 [4110/9880 (42%)]\t training loss: 16.082666\n",
      "epoch: 1 [4140/9880 (42%)]\t training loss: 7.680994\n",
      "epoch: 1 [4170/9880 (42%)]\t training loss: 16.401836\n",
      "epoch: 1 [4200/9880 (43%)]\t training loss: 9.976606\n",
      "epoch: 1 [4230/9880 (43%)]\t training loss: 8.632053\n",
      "epoch: 1 [4260/9880 (43%)]\t training loss: 8.040346\n",
      "epoch: 1 [4290/9880 (43%)]\t training loss: 5.142889\n",
      "epoch: 1 [4320/9880 (44%)]\t training loss: 9.641765\n",
      "epoch: 1 [4350/9880 (44%)]\t training loss: 10.972775\n",
      "epoch: 1 [4380/9880 (44%)]\t training loss: 10.955795\n",
      "epoch: 1 [4410/9880 (45%)]\t training loss: 8.583580\n",
      "epoch: 1 [4440/9880 (45%)]\t training loss: 15.105591\n",
      "epoch: 1 [4470/9880 (45%)]\t training loss: 13.844723\n",
      "epoch: 1 [4500/9880 (46%)]\t training loss: 17.532391\n",
      "epoch: 1 [4530/9880 (46%)]\t training loss: 9.672091\n",
      "epoch: 1 [4560/9880 (46%)]\t training loss: 18.598818\n",
      "epoch: 1 [4590/9880 (46%)]\t training loss: 6.917779\n",
      "epoch: 1 [4620/9880 (47%)]\t training loss: 28.138905\n",
      "epoch: 1 [4650/9880 (47%)]\t training loss: 5.930748\n",
      "epoch: 1 [4680/9880 (47%)]\t training loss: 11.630216\n",
      "epoch: 1 [4710/9880 (48%)]\t training loss: 12.058069\n",
      "epoch: 1 [4740/9880 (48%)]\t training loss: 10.698036\n",
      "epoch: 1 [4770/9880 (48%)]\t training loss: 7.608753\n",
      "epoch: 1 [4800/9880 (49%)]\t training loss: 15.323572\n",
      "epoch: 1 [4830/9880 (49%)]\t training loss: 10.889568\n",
      "epoch: 1 [4860/9880 (49%)]\t training loss: 8.258172\n",
      "epoch: 1 [4890/9880 (49%)]\t training loss: 9.932403\n",
      "epoch: 1 [4920/9880 (50%)]\t training loss: 7.274801\n",
      "epoch: 1 [4950/9880 (50%)]\t training loss: 13.254516\n",
      "epoch: 1 [4980/9880 (50%)]\t training loss: 7.444981\n",
      "epoch: 1 [5010/9880 (51%)]\t training loss: 10.281551\n",
      "epoch: 1 [5040/9880 (51%)]\t training loss: 8.097642\n",
      "epoch: 1 [5070/9880 (51%)]\t training loss: 17.747803\n",
      "epoch: 1 [5100/9880 (52%)]\t training loss: 10.306988\n",
      "epoch: 1 [5130/9880 (52%)]\t training loss: 5.704648\n",
      "epoch: 1 [5160/9880 (52%)]\t training loss: 19.533976\n",
      "epoch: 1 [5190/9880 (53%)]\t training loss: 15.179857\n",
      "epoch: 1 [5220/9880 (53%)]\t training loss: 10.950953\n",
      "epoch: 1 [5250/9880 (53%)]\t training loss: 17.658829\n",
      "epoch: 1 [5280/9880 (53%)]\t training loss: 12.106551\n",
      "epoch: 1 [5310/9880 (54%)]\t training loss: 9.173344\n",
      "epoch: 1 [5340/9880 (54%)]\t training loss: 14.750200\n",
      "epoch: 1 [5370/9880 (54%)]\t training loss: 18.233555\n",
      "epoch: 1 [5400/9880 (55%)]\t training loss: 11.509821\n",
      "epoch: 1 [5430/9880 (55%)]\t training loss: 11.075525\n",
      "epoch: 1 [5460/9880 (55%)]\t training loss: 20.527544\n",
      "epoch: 1 [5490/9880 (56%)]\t training loss: 17.879715\n",
      "epoch: 1 [5520/9880 (56%)]\t training loss: 9.294708\n",
      "epoch: 1 [5550/9880 (56%)]\t training loss: 10.743263\n",
      "epoch: 1 [5580/9880 (56%)]\t training loss: 9.869963\n",
      "epoch: 1 [5610/9880 (57%)]\t training loss: 13.577026\n",
      "epoch: 1 [5640/9880 (57%)]\t training loss: 11.788286\n",
      "epoch: 1 [5670/9880 (57%)]\t training loss: 10.849085\n",
      "epoch: 1 [5700/9880 (58%)]\t training loss: 7.508401\n",
      "epoch: 1 [5730/9880 (58%)]\t training loss: 14.567644\n",
      "epoch: 1 [5760/9880 (58%)]\t training loss: 22.658579\n",
      "epoch: 1 [5790/9880 (59%)]\t training loss: 6.843228\n",
      "epoch: 1 [5820/9880 (59%)]\t training loss: 9.697881\n",
      "epoch: 1 [5850/9880 (59%)]\t training loss: 13.362928\n",
      "epoch: 1 [5880/9880 (60%)]\t training loss: 28.231403\n",
      "epoch: 1 [5910/9880 (60%)]\t training loss: 14.782045\n",
      "epoch: 1 [5940/9880 (60%)]\t training loss: 8.470572\n",
      "epoch: 1 [5970/9880 (60%)]\t training loss: 15.416885\n",
      "epoch: 1 [6000/9880 (61%)]\t training loss: 9.262571\n",
      "epoch: 1 [6030/9880 (61%)]\t training loss: 16.734571\n",
      "epoch: 1 [6060/9880 (61%)]\t training loss: 7.286157\n",
      "epoch: 1 [6090/9880 (62%)]\t training loss: 8.692955\n",
      "epoch: 1 [6120/9880 (62%)]\t training loss: 7.735671\n",
      "epoch: 1 [6150/9880 (62%)]\t training loss: 6.284037\n",
      "epoch: 1 [6180/9880 (63%)]\t training loss: 10.496801\n",
      "epoch: 1 [6210/9880 (63%)]\t training loss: 9.648752\n",
      "epoch: 1 [6240/9880 (63%)]\t training loss: 10.451675\n",
      "epoch: 1 [6270/9880 (63%)]\t training loss: 18.658983\n",
      "epoch: 1 [6300/9880 (64%)]\t training loss: 7.199428\n",
      "epoch: 1 [6330/9880 (64%)]\t training loss: 8.281114\n",
      "epoch: 1 [6360/9880 (64%)]\t training loss: 3.800975\n",
      "epoch: 1 [6390/9880 (65%)]\t training loss: 8.325527\n",
      "epoch: 1 [6420/9880 (65%)]\t training loss: 7.651560\n",
      "epoch: 1 [6450/9880 (65%)]\t training loss: 5.545731\n",
      "epoch: 1 [6480/9880 (66%)]\t training loss: 16.148302\n",
      "epoch: 1 [6510/9880 (66%)]\t training loss: 16.910841\n",
      "epoch: 1 [6540/9880 (66%)]\t training loss: 6.309379\n",
      "epoch: 1 [6570/9880 (66%)]\t training loss: 5.643649\n",
      "epoch: 1 [6600/9880 (67%)]\t training loss: 15.020683\n",
      "epoch: 1 [6630/9880 (67%)]\t training loss: 26.172796\n",
      "epoch: 1 [6660/9880 (67%)]\t training loss: 4.021697\n",
      "epoch: 1 [6690/9880 (68%)]\t training loss: 10.225864\n",
      "epoch: 1 [6720/9880 (68%)]\t training loss: 15.840242\n",
      "epoch: 1 [6750/9880 (68%)]\t training loss: 12.854209\n",
      "epoch: 1 [6780/9880 (69%)]\t training loss: 13.416729\n",
      "epoch: 1 [6810/9880 (69%)]\t training loss: 18.565651\n",
      "epoch: 1 [6840/9880 (69%)]\t training loss: 10.533483\n",
      "epoch: 1 [6870/9880 (70%)]\t training loss: 15.148117\n",
      "epoch: 1 [6900/9880 (70%)]\t training loss: 6.959878\n",
      "epoch: 1 [6930/9880 (70%)]\t training loss: 11.061296\n",
      "epoch: 1 [6960/9880 (70%)]\t training loss: 7.587196\n",
      "epoch: 1 [6990/9880 (71%)]\t training loss: 9.317850\n",
      "epoch: 1 [7020/9880 (71%)]\t training loss: 10.041961\n",
      "epoch: 1 [7050/9880 (71%)]\t training loss: 13.102313\n",
      "epoch: 1 [7080/9880 (72%)]\t training loss: 17.583136\n",
      "epoch: 1 [7110/9880 (72%)]\t training loss: 11.464602\n",
      "epoch: 1 [7140/9880 (72%)]\t training loss: 2.663808\n",
      "epoch: 1 [7170/9880 (73%)]\t training loss: 7.416978\n",
      "epoch: 1 [7200/9880 (73%)]\t training loss: 5.259010\n",
      "epoch: 1 [7230/9880 (73%)]\t training loss: 13.960588\n",
      "epoch: 1 [7260/9880 (73%)]\t training loss: 18.039976\n",
      "epoch: 1 [7290/9880 (74%)]\t training loss: 21.658159\n",
      "epoch: 1 [7320/9880 (74%)]\t training loss: 19.894444\n",
      "epoch: 1 [7350/9880 (74%)]\t training loss: 9.889445\n",
      "epoch: 1 [7380/9880 (75%)]\t training loss: 6.309620\n",
      "epoch: 1 [7410/9880 (75%)]\t training loss: 14.340840\n",
      "epoch: 1 [7440/9880 (75%)]\t training loss: 8.917154\n",
      "epoch: 1 [7470/9880 (76%)]\t training loss: 15.689455\n",
      "epoch: 1 [7500/9880 (76%)]\t training loss: 12.323515\n",
      "epoch: 1 [7530/9880 (76%)]\t training loss: 15.683796\n",
      "epoch: 1 [7560/9880 (77%)]\t training loss: 12.279955\n",
      "epoch: 1 [7590/9880 (77%)]\t training loss: 12.287724\n",
      "epoch: 1 [7620/9880 (77%)]\t training loss: 8.234359\n",
      "epoch: 1 [7650/9880 (77%)]\t training loss: 4.807731\n",
      "epoch: 1 [7680/9880 (78%)]\t training loss: 11.630483\n",
      "epoch: 1 [7710/9880 (78%)]\t training loss: 6.122480\n",
      "epoch: 1 [7740/9880 (78%)]\t training loss: 8.655100\n",
      "epoch: 1 [7770/9880 (79%)]\t training loss: 10.916930\n",
      "epoch: 1 [7800/9880 (79%)]\t training loss: 14.118102\n",
      "epoch: 1 [7830/9880 (79%)]\t training loss: 3.529926\n",
      "epoch: 1 [7860/9880 (80%)]\t training loss: 16.206831\n",
      "epoch: 1 [7890/9880 (80%)]\t training loss: 3.786229\n",
      "epoch: 1 [7920/9880 (80%)]\t training loss: 13.683680\n",
      "epoch: 1 [7950/9880 (80%)]\t training loss: 8.392153\n",
      "epoch: 1 [7980/9880 (81%)]\t training loss: 10.829588\n",
      "epoch: 1 [8010/9880 (81%)]\t training loss: 10.695623\n",
      "epoch: 1 [8040/9880 (81%)]\t training loss: 6.859055\n",
      "epoch: 1 [8070/9880 (82%)]\t training loss: 18.933397\n",
      "epoch: 1 [8100/9880 (82%)]\t training loss: 16.038296\n",
      "epoch: 1 [8130/9880 (82%)]\t training loss: 8.873886\n",
      "epoch: 1 [8160/9880 (83%)]\t training loss: 3.991100\n",
      "epoch: 1 [8190/9880 (83%)]\t training loss: 6.656060\n",
      "epoch: 1 [8220/9880 (83%)]\t training loss: 8.595097\n",
      "epoch: 1 [8250/9880 (83%)]\t training loss: 13.037223\n",
      "epoch: 1 [8280/9880 (84%)]\t training loss: 19.561508\n",
      "epoch: 1 [8310/9880 (84%)]\t training loss: 14.615004\n",
      "epoch: 1 [8340/9880 (84%)]\t training loss: 12.874516\n",
      "epoch: 1 [8370/9880 (85%)]\t training loss: 17.548380\n",
      "epoch: 1 [8400/9880 (85%)]\t training loss: 2.211238\n",
      "epoch: 1 [8430/9880 (85%)]\t training loss: 5.811275\n",
      "epoch: 1 [8460/9880 (86%)]\t training loss: 10.266479\n",
      "epoch: 1 [8490/9880 (86%)]\t training loss: 14.188990\n",
      "epoch: 1 [8520/9880 (86%)]\t training loss: 10.467291\n",
      "epoch: 1 [8550/9880 (87%)]\t training loss: 23.752514\n",
      "epoch: 1 [8580/9880 (87%)]\t training loss: 8.930861\n",
      "epoch: 1 [8610/9880 (87%)]\t training loss: 14.980726\n",
      "epoch: 1 [8640/9880 (87%)]\t training loss: 21.130928\n",
      "epoch: 1 [8670/9880 (88%)]\t training loss: 15.109684\n",
      "epoch: 1 [8700/9880 (88%)]\t training loss: 9.982703\n",
      "epoch: 1 [8730/9880 (88%)]\t training loss: 15.277421\n",
      "epoch: 1 [8760/9880 (89%)]\t training loss: 12.263657\n",
      "epoch: 1 [8790/9880 (89%)]\t training loss: 22.540304\n",
      "epoch: 1 [8820/9880 (89%)]\t training loss: 12.353423\n",
      "epoch: 1 [8850/9880 (90%)]\t training loss: 9.058003\n",
      "epoch: 1 [8880/9880 (90%)]\t training loss: 14.341816\n",
      "epoch: 1 [8910/9880 (90%)]\t training loss: 8.184987\n",
      "epoch: 1 [8940/9880 (90%)]\t training loss: 15.950214\n",
      "epoch: 1 [8970/9880 (91%)]\t training loss: 16.149410\n",
      "epoch: 1 [9000/9880 (91%)]\t training loss: 18.765200\n",
      "epoch: 1 [9030/9880 (91%)]\t training loss: 10.042023\n",
      "epoch: 1 [9060/9880 (92%)]\t training loss: 16.277809\n",
      "epoch: 1 [9090/9880 (92%)]\t training loss: 15.409376\n",
      "epoch: 1 [9120/9880 (92%)]\t training loss: 7.075315\n",
      "epoch: 1 [9150/9880 (93%)]\t training loss: 16.520535\n",
      "epoch: 1 [9180/9880 (93%)]\t training loss: 8.004404\n",
      "epoch: 1 [9210/9880 (93%)]\t training loss: 16.747015\n",
      "epoch: 1 [9240/9880 (94%)]\t training loss: 17.616009\n",
      "epoch: 1 [9270/9880 (94%)]\t training loss: 4.446514\n",
      "epoch: 1 [9300/9880 (94%)]\t training loss: 14.639228\n",
      "epoch: 1 [9330/9880 (94%)]\t training loss: 20.865955\n",
      "epoch: 1 [9360/9880 (95%)]\t training loss: 7.379112\n",
      "epoch: 1 [9390/9880 (95%)]\t training loss: 6.384724\n",
      "epoch: 1 [9420/9880 (95%)]\t training loss: 20.653296\n",
      "epoch: 1 [9450/9880 (96%)]\t training loss: 6.022191\n",
      "epoch: 1 [9480/9880 (96%)]\t training loss: 12.681948\n",
      "epoch: 1 [9510/9880 (96%)]\t training loss: 9.294298\n",
      "epoch: 1 [9540/9880 (97%)]\t training loss: 5.093425\n",
      "epoch: 1 [9570/9880 (97%)]\t training loss: 13.985089\n",
      "epoch: 1 [9600/9880 (97%)]\t training loss: 10.232212\n",
      "epoch: 1 [9630/9880 (97%)]\t training loss: 15.852852\n",
      "epoch: 1 [9660/9880 (98%)]\t training loss: 19.400280\n",
      "epoch: 1 [9690/9880 (98%)]\t training loss: 8.420694\n",
      "epoch: 1 [9720/9880 (98%)]\t training loss: 8.928583\n",
      "epoch: 1 [9750/9880 (99%)]\t training loss: 13.605371\n",
      "epoch: 1 [9780/9880 (99%)]\t training loss: 15.253289\n",
      "epoch: 1 [9810/9880 (99%)]\t training loss: 15.311951\n",
      "epoch: 1 [9840/9880 (100%)]\t training loss: 19.504766\n",
      "epoch: 1 [9870/9880 (100%)]\t training loss: 15.990709\n",
      "\n",
      "Test dataset: Overall Loss: 4.2063,  (9880%)\n",
      "\n",
      "epoch: 2 [0/9880 (0%)]\t training loss: 5.986109\n",
      "epoch: 2 [30/9880 (0%)]\t training loss: 17.178532\n",
      "epoch: 2 [60/9880 (1%)]\t training loss: 10.760271\n",
      "epoch: 2 [90/9880 (1%)]\t training loss: 18.954411\n",
      "epoch: 2 [120/9880 (1%)]\t training loss: 19.475876\n",
      "epoch: 2 [150/9880 (2%)]\t training loss: 10.139132\n",
      "epoch: 2 [180/9880 (2%)]\t training loss: 8.035997\n",
      "epoch: 2 [210/9880 (2%)]\t training loss: 9.544785\n",
      "epoch: 2 [240/9880 (2%)]\t training loss: 10.822340\n",
      "epoch: 2 [270/9880 (3%)]\t training loss: 9.351210\n",
      "epoch: 2 [300/9880 (3%)]\t training loss: 10.903551\n",
      "epoch: 2 [330/9880 (3%)]\t training loss: 11.164101\n",
      "epoch: 2 [360/9880 (4%)]\t training loss: 8.275461\n",
      "epoch: 2 [390/9880 (4%)]\t training loss: 17.132410\n",
      "epoch: 2 [420/9880 (4%)]\t training loss: 11.575703\n",
      "epoch: 2 [450/9880 (5%)]\t training loss: 16.324886\n",
      "epoch: 2 [480/9880 (5%)]\t training loss: 19.011261\n",
      "epoch: 2 [510/9880 (5%)]\t training loss: 12.057508\n",
      "epoch: 2 [540/9880 (5%)]\t training loss: 15.719711\n",
      "epoch: 2 [570/9880 (6%)]\t training loss: 18.821903\n",
      "epoch: 2 [600/9880 (6%)]\t training loss: 18.966101\n",
      "epoch: 2 [630/9880 (6%)]\t training loss: 10.529493\n",
      "epoch: 2 [660/9880 (7%)]\t training loss: 12.024254\n",
      "epoch: 2 [690/9880 (7%)]\t training loss: 10.279812\n",
      "epoch: 2 [720/9880 (7%)]\t training loss: 11.068256\n",
      "epoch: 2 [750/9880 (8%)]\t training loss: 12.636562\n",
      "epoch: 2 [780/9880 (8%)]\t training loss: 20.756334\n",
      "epoch: 2 [810/9880 (8%)]\t training loss: 9.515231\n",
      "epoch: 2 [840/9880 (9%)]\t training loss: 7.637177\n",
      "epoch: 2 [870/9880 (9%)]\t training loss: 12.756816\n",
      "epoch: 2 [900/9880 (9%)]\t training loss: 13.248646\n",
      "epoch: 2 [930/9880 (9%)]\t training loss: 16.395714\n",
      "epoch: 2 [960/9880 (10%)]\t training loss: 9.879370\n",
      "epoch: 2 [990/9880 (10%)]\t training loss: 7.285958\n",
      "epoch: 2 [1020/9880 (10%)]\t training loss: 16.835869\n",
      "epoch: 2 [1050/9880 (11%)]\t training loss: 10.114248\n",
      "epoch: 2 [1080/9880 (11%)]\t training loss: 7.126186\n",
      "epoch: 2 [1110/9880 (11%)]\t training loss: 19.096884\n",
      "epoch: 2 [1140/9880 (12%)]\t training loss: 14.781300\n",
      "epoch: 2 [1170/9880 (12%)]\t training loss: 11.907305\n",
      "epoch: 2 [1200/9880 (12%)]\t training loss: 8.581562\n",
      "epoch: 2 [1230/9880 (12%)]\t training loss: 13.225624\n",
      "epoch: 2 [1260/9880 (13%)]\t training loss: 10.985653\n",
      "epoch: 2 [1290/9880 (13%)]\t training loss: 7.230814\n",
      "epoch: 2 [1320/9880 (13%)]\t training loss: 8.365017\n",
      "epoch: 2 [1350/9880 (14%)]\t training loss: 5.275953\n",
      "epoch: 2 [1380/9880 (14%)]\t training loss: 6.349766\n",
      "epoch: 2 [1410/9880 (14%)]\t training loss: 13.744815\n",
      "epoch: 2 [1440/9880 (15%)]\t training loss: 10.497255\n",
      "epoch: 2 [1470/9880 (15%)]\t training loss: 9.120447\n",
      "epoch: 2 [1500/9880 (15%)]\t training loss: 14.158594\n",
      "epoch: 2 [1530/9880 (15%)]\t training loss: 18.938278\n",
      "epoch: 2 [1560/9880 (16%)]\t training loss: 9.281477\n",
      "epoch: 2 [1590/9880 (16%)]\t training loss: 17.498972\n",
      "epoch: 2 [1620/9880 (16%)]\t training loss: 10.326361\n",
      "epoch: 2 [1650/9880 (17%)]\t training loss: 10.627810\n",
      "epoch: 2 [1680/9880 (17%)]\t training loss: 6.295508\n",
      "epoch: 2 [1710/9880 (17%)]\t training loss: 17.901236\n",
      "epoch: 2 [1740/9880 (18%)]\t training loss: 13.213442\n",
      "epoch: 2 [1770/9880 (18%)]\t training loss: 16.919680\n",
      "epoch: 2 [1800/9880 (18%)]\t training loss: 10.579515\n",
      "epoch: 2 [1830/9880 (19%)]\t training loss: 16.606798\n",
      "epoch: 2 [1860/9880 (19%)]\t training loss: 11.014757\n",
      "epoch: 2 [1890/9880 (19%)]\t training loss: 9.427496\n",
      "epoch: 2 [1920/9880 (19%)]\t training loss: 8.831299\n",
      "epoch: 2 [1950/9880 (20%)]\t training loss: 9.269729\n",
      "epoch: 2 [1980/9880 (20%)]\t training loss: 10.390827\n",
      "epoch: 2 [2010/9880 (20%)]\t training loss: 18.293087\n",
      "epoch: 2 [2040/9880 (21%)]\t training loss: 11.006798\n",
      "epoch: 2 [2070/9880 (21%)]\t training loss: 15.242711\n",
      "epoch: 2 [2100/9880 (21%)]\t training loss: 10.750496\n",
      "epoch: 2 [2130/9880 (22%)]\t training loss: 9.095898\n",
      "epoch: 2 [2160/9880 (22%)]\t training loss: 15.234247\n",
      "epoch: 2 [2190/9880 (22%)]\t training loss: 8.631233\n",
      "epoch: 2 [2220/9880 (22%)]\t training loss: 13.429315\n",
      "epoch: 2 [2250/9880 (23%)]\t training loss: 10.609200\n",
      "epoch: 2 [2280/9880 (23%)]\t training loss: 12.473164\n",
      "epoch: 2 [2310/9880 (23%)]\t training loss: 9.503401\n",
      "epoch: 2 [2340/9880 (24%)]\t training loss: 8.948341\n",
      "epoch: 2 [2370/9880 (24%)]\t training loss: 20.091217\n",
      "epoch: 2 [2400/9880 (24%)]\t training loss: 13.257441\n",
      "epoch: 2 [2430/9880 (25%)]\t training loss: 13.987991\n",
      "epoch: 2 [2460/9880 (25%)]\t training loss: 11.548235\n",
      "epoch: 2 [2490/9880 (25%)]\t training loss: 10.103930\n",
      "epoch: 2 [2520/9880 (26%)]\t training loss: 8.941263\n",
      "epoch: 2 [2550/9880 (26%)]\t training loss: 9.299395\n",
      "epoch: 2 [2580/9880 (26%)]\t training loss: 15.057287\n",
      "epoch: 2 [2610/9880 (26%)]\t training loss: 5.784742\n",
      "epoch: 2 [2640/9880 (27%)]\t training loss: 20.286425\n",
      "epoch: 2 [2670/9880 (27%)]\t training loss: 6.072100\n",
      "epoch: 2 [2700/9880 (27%)]\t training loss: 16.015160\n",
      "epoch: 2 [2730/9880 (28%)]\t training loss: 12.543020\n",
      "epoch: 2 [2760/9880 (28%)]\t training loss: 6.538698\n",
      "epoch: 2 [2790/9880 (28%)]\t training loss: 18.309845\n",
      "epoch: 2 [2820/9880 (29%)]\t training loss: 12.328178\n",
      "epoch: 2 [2850/9880 (29%)]\t training loss: 21.712879\n",
      "epoch: 2 [2880/9880 (29%)]\t training loss: 14.788897\n",
      "epoch: 2 [2910/9880 (29%)]\t training loss: 10.130587\n",
      "epoch: 2 [2940/9880 (30%)]\t training loss: 16.261177\n",
      "epoch: 2 [2970/9880 (30%)]\t training loss: 9.924699\n",
      "epoch: 2 [3000/9880 (30%)]\t training loss: 18.887012\n",
      "epoch: 2 [3030/9880 (31%)]\t training loss: 5.641868\n",
      "epoch: 2 [3060/9880 (31%)]\t training loss: 7.572940\n",
      "epoch: 2 [3090/9880 (31%)]\t training loss: 13.705830\n",
      "epoch: 2 [3120/9880 (32%)]\t training loss: 11.277874\n",
      "epoch: 2 [3150/9880 (32%)]\t training loss: 20.409031\n",
      "epoch: 2 [3180/9880 (32%)]\t training loss: 16.727400\n",
      "epoch: 2 [3210/9880 (32%)]\t training loss: 16.684500\n",
      "epoch: 2 [3240/9880 (33%)]\t training loss: 5.685047\n",
      "epoch: 2 [3270/9880 (33%)]\t training loss: 19.052654\n",
      "epoch: 2 [3300/9880 (33%)]\t training loss: 6.901464\n",
      "epoch: 2 [3330/9880 (34%)]\t training loss: 3.739679\n",
      "epoch: 2 [3360/9880 (34%)]\t training loss: 18.904444\n",
      "epoch: 2 [3390/9880 (34%)]\t training loss: 18.326649\n",
      "epoch: 2 [3420/9880 (35%)]\t training loss: 12.783784\n",
      "epoch: 2 [3450/9880 (35%)]\t training loss: 15.202879\n",
      "epoch: 2 [3480/9880 (35%)]\t training loss: 13.969851\n",
      "epoch: 2 [3510/9880 (36%)]\t training loss: 13.950098\n",
      "epoch: 2 [3540/9880 (36%)]\t training loss: 6.973712\n",
      "epoch: 2 [3570/9880 (36%)]\t training loss: 12.204597\n",
      "epoch: 2 [3600/9880 (36%)]\t training loss: 11.478582\n",
      "epoch: 2 [3630/9880 (37%)]\t training loss: 19.955658\n",
      "epoch: 2 [3660/9880 (37%)]\t training loss: 9.960658\n",
      "epoch: 2 [3690/9880 (37%)]\t training loss: 14.206844\n",
      "epoch: 2 [3720/9880 (38%)]\t training loss: 6.500618\n",
      "epoch: 2 [3750/9880 (38%)]\t training loss: 15.477129\n",
      "epoch: 2 [3780/9880 (38%)]\t training loss: 20.592300\n",
      "epoch: 2 [3810/9880 (39%)]\t training loss: 13.143004\n",
      "epoch: 2 [3840/9880 (39%)]\t training loss: 17.163464\n",
      "epoch: 2 [3870/9880 (39%)]\t training loss: 2.407114\n",
      "epoch: 2 [3900/9880 (39%)]\t training loss: 15.113241\n",
      "epoch: 2 [3930/9880 (40%)]\t training loss: 12.334778\n",
      "epoch: 2 [3960/9880 (40%)]\t training loss: 14.646507\n",
      "epoch: 2 [3990/9880 (40%)]\t training loss: 18.079283\n",
      "epoch: 2 [4020/9880 (41%)]\t training loss: 8.905649\n",
      "epoch: 2 [4050/9880 (41%)]\t training loss: 21.633152\n",
      "epoch: 2 [4080/9880 (41%)]\t training loss: 14.909911\n",
      "epoch: 2 [4110/9880 (42%)]\t training loss: 13.702179\n",
      "epoch: 2 [4140/9880 (42%)]\t training loss: 20.306042\n",
      "epoch: 2 [4170/9880 (42%)]\t training loss: 12.268047\n",
      "epoch: 2 [4200/9880 (43%)]\t training loss: 13.808454\n",
      "epoch: 2 [4230/9880 (43%)]\t training loss: 11.992495\n",
      "epoch: 2 [4260/9880 (43%)]\t training loss: 16.539207\n",
      "epoch: 2 [4290/9880 (43%)]\t training loss: 9.837730\n",
      "epoch: 2 [4320/9880 (44%)]\t training loss: 18.309988\n",
      "epoch: 2 [4350/9880 (44%)]\t training loss: 12.412889\n",
      "epoch: 2 [4380/9880 (44%)]\t training loss: 13.377157\n",
      "epoch: 2 [4410/9880 (45%)]\t training loss: 16.125181\n",
      "epoch: 2 [4440/9880 (45%)]\t training loss: 10.451075\n",
      "epoch: 2 [4470/9880 (45%)]\t training loss: 6.488141\n",
      "epoch: 2 [4500/9880 (46%)]\t training loss: 13.573696\n",
      "epoch: 2 [4530/9880 (46%)]\t training loss: 6.141516\n",
      "epoch: 2 [4560/9880 (46%)]\t training loss: 18.148615\n",
      "epoch: 2 [4590/9880 (46%)]\t training loss: 14.676322\n",
      "epoch: 2 [4620/9880 (47%)]\t training loss: 14.142401\n",
      "epoch: 2 [4650/9880 (47%)]\t training loss: 12.601831\n",
      "epoch: 2 [4680/9880 (47%)]\t training loss: 8.809868\n",
      "epoch: 2 [4710/9880 (48%)]\t training loss: 12.745770\n",
      "epoch: 2 [4740/9880 (48%)]\t training loss: 9.492523\n",
      "epoch: 2 [4770/9880 (48%)]\t training loss: 22.122595\n",
      "epoch: 2 [4800/9880 (49%)]\t training loss: 10.213199\n",
      "epoch: 2 [4830/9880 (49%)]\t training loss: 11.807930\n",
      "epoch: 2 [4860/9880 (49%)]\t training loss: 14.564702\n",
      "epoch: 2 [4890/9880 (49%)]\t training loss: 18.820024\n",
      "epoch: 2 [4920/9880 (50%)]\t training loss: 14.326236\n",
      "epoch: 2 [4950/9880 (50%)]\t training loss: 15.855553\n",
      "epoch: 2 [4980/9880 (50%)]\t training loss: 25.094950\n",
      "epoch: 2 [5010/9880 (51%)]\t training loss: 10.188376\n",
      "epoch: 2 [5040/9880 (51%)]\t training loss: 11.610605\n",
      "epoch: 2 [5070/9880 (51%)]\t training loss: 11.369465\n",
      "epoch: 2 [5100/9880 (52%)]\t training loss: 11.055503\n",
      "epoch: 2 [5130/9880 (52%)]\t training loss: 12.448153\n",
      "epoch: 2 [5160/9880 (52%)]\t training loss: 9.110913\n",
      "epoch: 2 [5190/9880 (53%)]\t training loss: 9.320806\n",
      "epoch: 2 [5220/9880 (53%)]\t training loss: 13.401878\n",
      "epoch: 2 [5250/9880 (53%)]\t training loss: 12.521470\n",
      "epoch: 2 [5280/9880 (53%)]\t training loss: 17.067627\n",
      "epoch: 2 [5310/9880 (54%)]\t training loss: 16.496567\n",
      "epoch: 2 [5340/9880 (54%)]\t training loss: 10.166082\n",
      "epoch: 2 [5370/9880 (54%)]\t training loss: 10.378477\n",
      "epoch: 2 [5400/9880 (55%)]\t training loss: 19.013390\n",
      "epoch: 2 [5430/9880 (55%)]\t training loss: 18.764614\n",
      "epoch: 2 [5460/9880 (55%)]\t training loss: 16.589481\n",
      "epoch: 2 [5490/9880 (56%)]\t training loss: 10.954445\n",
      "epoch: 2 [5520/9880 (56%)]\t training loss: 8.757477\n",
      "epoch: 2 [5550/9880 (56%)]\t training loss: 15.492479\n",
      "epoch: 2 [5580/9880 (56%)]\t training loss: 12.120226\n",
      "epoch: 2 [5610/9880 (57%)]\t training loss: 7.405262\n",
      "epoch: 2 [5640/9880 (57%)]\t training loss: 10.041336\n",
      "epoch: 2 [5670/9880 (57%)]\t training loss: 6.500074\n",
      "epoch: 2 [5700/9880 (58%)]\t training loss: 11.420468\n",
      "epoch: 2 [5730/9880 (58%)]\t training loss: 7.281297\n",
      "epoch: 2 [5760/9880 (58%)]\t training loss: 12.634141\n",
      "epoch: 2 [5790/9880 (59%)]\t training loss: 4.636013\n",
      "epoch: 2 [5820/9880 (59%)]\t training loss: 8.755483\n",
      "epoch: 2 [5850/9880 (59%)]\t training loss: 15.901289\n",
      "epoch: 2 [5880/9880 (60%)]\t training loss: 14.733844\n",
      "epoch: 2 [5910/9880 (60%)]\t training loss: 19.565201\n",
      "epoch: 2 [5940/9880 (60%)]\t training loss: 16.151403\n",
      "epoch: 2 [5970/9880 (60%)]\t training loss: 15.142282\n",
      "epoch: 2 [6000/9880 (61%)]\t training loss: 8.253066\n",
      "epoch: 2 [6030/9880 (61%)]\t training loss: 17.511724\n",
      "epoch: 2 [6060/9880 (61%)]\t training loss: 18.905798\n",
      "epoch: 2 [6090/9880 (62%)]\t training loss: 10.476968\n",
      "epoch: 2 [6120/9880 (62%)]\t training loss: 16.500769\n",
      "epoch: 2 [6150/9880 (62%)]\t training loss: 7.423563\n",
      "epoch: 2 [6180/9880 (63%)]\t training loss: 9.541183\n",
      "epoch: 2 [6210/9880 (63%)]\t training loss: 12.648266\n",
      "epoch: 2 [6240/9880 (63%)]\t training loss: 13.665119\n",
      "epoch: 2 [6270/9880 (63%)]\t training loss: 14.760191\n",
      "epoch: 2 [6300/9880 (64%)]\t training loss: 9.672737\n",
      "epoch: 2 [6330/9880 (64%)]\t training loss: 7.934097\n",
      "epoch: 2 [6360/9880 (64%)]\t training loss: 7.607487\n",
      "epoch: 2 [6390/9880 (65%)]\t training loss: 15.426273\n",
      "epoch: 2 [6420/9880 (65%)]\t training loss: 13.008385\n",
      "epoch: 2 [6450/9880 (65%)]\t training loss: 9.072195\n",
      "epoch: 2 [6480/9880 (66%)]\t training loss: 11.870893\n",
      "epoch: 2 [6510/9880 (66%)]\t training loss: 8.533215\n",
      "epoch: 2 [6540/9880 (66%)]\t training loss: 14.045679\n",
      "epoch: 2 [6570/9880 (66%)]\t training loss: 7.886743\n",
      "epoch: 2 [6600/9880 (67%)]\t training loss: 14.311132\n",
      "epoch: 2 [6630/9880 (67%)]\t training loss: 13.579765\n",
      "epoch: 2 [6660/9880 (67%)]\t training loss: 18.803356\n",
      "epoch: 2 [6690/9880 (68%)]\t training loss: 18.978539\n",
      "epoch: 2 [6720/9880 (68%)]\t training loss: 14.236707\n",
      "epoch: 2 [6750/9880 (68%)]\t training loss: 13.338430\n",
      "epoch: 2 [6780/9880 (69%)]\t training loss: 15.689619\n",
      "epoch: 2 [6810/9880 (69%)]\t training loss: 25.478762\n",
      "epoch: 2 [6840/9880 (69%)]\t training loss: 18.105116\n",
      "epoch: 2 [6870/9880 (70%)]\t training loss: 12.068953\n",
      "epoch: 2 [6900/9880 (70%)]\t training loss: 11.723116\n",
      "epoch: 2 [6930/9880 (70%)]\t training loss: 14.877459\n",
      "epoch: 2 [6960/9880 (70%)]\t training loss: 16.655739\n",
      "epoch: 2 [6990/9880 (71%)]\t training loss: 13.425851\n",
      "epoch: 2 [7020/9880 (71%)]\t training loss: 15.581179\n",
      "epoch: 2 [7050/9880 (71%)]\t training loss: 6.338420\n",
      "epoch: 2 [7080/9880 (72%)]\t training loss: 11.504602\n",
      "epoch: 2 [7110/9880 (72%)]\t training loss: 24.249458\n",
      "epoch: 2 [7140/9880 (72%)]\t training loss: 8.993697\n",
      "epoch: 2 [7170/9880 (73%)]\t training loss: 11.184145\n",
      "epoch: 2 [7200/9880 (73%)]\t training loss: 17.597614\n",
      "epoch: 2 [7230/9880 (73%)]\t training loss: 9.562429\n",
      "epoch: 2 [7260/9880 (73%)]\t training loss: 17.953648\n",
      "epoch: 2 [7290/9880 (74%)]\t training loss: 10.922365\n",
      "epoch: 2 [7320/9880 (74%)]\t training loss: 9.180626\n",
      "epoch: 2 [7350/9880 (74%)]\t training loss: 16.962721\n",
      "epoch: 2 [7380/9880 (75%)]\t training loss: 12.933481\n",
      "epoch: 2 [7410/9880 (75%)]\t training loss: 8.789930\n",
      "epoch: 2 [7440/9880 (75%)]\t training loss: 19.107748\n",
      "epoch: 2 [7470/9880 (76%)]\t training loss: 9.073498\n",
      "epoch: 2 [7500/9880 (76%)]\t training loss: 5.163915\n",
      "epoch: 2 [7530/9880 (76%)]\t training loss: 8.549915\n",
      "epoch: 2 [7560/9880 (77%)]\t training loss: 12.111971\n",
      "epoch: 2 [7590/9880 (77%)]\t training loss: 11.634432\n",
      "epoch: 2 [7620/9880 (77%)]\t training loss: 11.430402\n",
      "epoch: 2 [7650/9880 (77%)]\t training loss: 16.732639\n",
      "epoch: 2 [7680/9880 (78%)]\t training loss: 10.232867\n",
      "epoch: 2 [7710/9880 (78%)]\t training loss: 11.950127\n",
      "epoch: 2 [7740/9880 (78%)]\t training loss: 16.713163\n",
      "epoch: 2 [7770/9880 (79%)]\t training loss: 10.321637\n",
      "epoch: 2 [7800/9880 (79%)]\t training loss: 17.249504\n",
      "epoch: 2 [7830/9880 (79%)]\t training loss: 20.503317\n",
      "epoch: 2 [7860/9880 (80%)]\t training loss: 5.801826\n",
      "epoch: 2 [7890/9880 (80%)]\t training loss: 5.090675\n",
      "epoch: 2 [7920/9880 (80%)]\t training loss: 5.300684\n",
      "epoch: 2 [7950/9880 (80%)]\t training loss: 7.728843\n",
      "epoch: 2 [7980/9880 (81%)]\t training loss: 14.902049\n",
      "epoch: 2 [8010/9880 (81%)]\t training loss: 7.228045\n",
      "epoch: 2 [8040/9880 (81%)]\t training loss: 16.030472\n",
      "epoch: 2 [8070/9880 (82%)]\t training loss: 13.346938\n",
      "epoch: 2 [8100/9880 (82%)]\t training loss: 7.079029\n",
      "epoch: 2 [8130/9880 (82%)]\t training loss: 16.113441\n",
      "epoch: 2 [8160/9880 (83%)]\t training loss: 13.604941\n",
      "epoch: 2 [8190/9880 (83%)]\t training loss: 5.848658\n",
      "epoch: 2 [8220/9880 (83%)]\t training loss: 17.833445\n",
      "epoch: 2 [8250/9880 (83%)]\t training loss: 12.431242\n",
      "epoch: 2 [8280/9880 (84%)]\t training loss: 7.445640\n",
      "epoch: 2 [8310/9880 (84%)]\t training loss: 8.692247\n",
      "epoch: 2 [8340/9880 (84%)]\t training loss: 27.822655\n",
      "epoch: 2 [8370/9880 (85%)]\t training loss: 8.139041\n",
      "epoch: 2 [8400/9880 (85%)]\t training loss: 10.715979\n",
      "epoch: 2 [8430/9880 (85%)]\t training loss: 9.198570\n",
      "epoch: 2 [8460/9880 (86%)]\t training loss: 9.388693\n",
      "epoch: 2 [8490/9880 (86%)]\t training loss: 9.740362\n",
      "epoch: 2 [8520/9880 (86%)]\t training loss: 8.068634\n",
      "epoch: 2 [8550/9880 (87%)]\t training loss: 16.959545\n",
      "epoch: 2 [8580/9880 (87%)]\t training loss: 12.615215\n",
      "epoch: 2 [8610/9880 (87%)]\t training loss: 13.047340\n",
      "epoch: 2 [8640/9880 (87%)]\t training loss: 8.394922\n",
      "epoch: 2 [8670/9880 (88%)]\t training loss: 12.726714\n",
      "epoch: 2 [8700/9880 (88%)]\t training loss: 14.824093\n",
      "epoch: 2 [8730/9880 (88%)]\t training loss: 14.840937\n",
      "epoch: 2 [8760/9880 (89%)]\t training loss: 12.106535\n",
      "epoch: 2 [8790/9880 (89%)]\t training loss: 12.990032\n",
      "epoch: 2 [8820/9880 (89%)]\t training loss: 4.920759\n",
      "epoch: 2 [8850/9880 (90%)]\t training loss: 12.369368\n",
      "epoch: 2 [8880/9880 (90%)]\t training loss: 10.518798\n",
      "epoch: 2 [8910/9880 (90%)]\t training loss: 23.042120\n",
      "epoch: 2 [8940/9880 (90%)]\t training loss: 10.544942\n",
      "epoch: 2 [8970/9880 (91%)]\t training loss: 12.799795\n",
      "epoch: 2 [9000/9880 (91%)]\t training loss: 19.819820\n",
      "epoch: 2 [9030/9880 (91%)]\t training loss: 19.695267\n",
      "epoch: 2 [9060/9880 (92%)]\t training loss: 11.249372\n",
      "epoch: 2 [9090/9880 (92%)]\t training loss: 10.727600\n",
      "epoch: 2 [9120/9880 (92%)]\t training loss: 11.871101\n",
      "epoch: 2 [9150/9880 (93%)]\t training loss: 8.032133\n",
      "epoch: 2 [9180/9880 (93%)]\t training loss: 15.617104\n",
      "epoch: 2 [9210/9880 (93%)]\t training loss: 7.613645\n",
      "epoch: 2 [9240/9880 (94%)]\t training loss: 7.929985\n",
      "epoch: 2 [9270/9880 (94%)]\t training loss: 10.640392\n",
      "epoch: 2 [9300/9880 (94%)]\t training loss: 9.119423\n",
      "epoch: 2 [9330/9880 (94%)]\t training loss: 13.233383\n",
      "epoch: 2 [9360/9880 (95%)]\t training loss: 7.483108\n",
      "epoch: 2 [9390/9880 (95%)]\t training loss: 11.273262\n",
      "epoch: 2 [9420/9880 (95%)]\t training loss: 22.744686\n",
      "epoch: 2 [9450/9880 (96%)]\t training loss: 10.238825\n",
      "epoch: 2 [9480/9880 (96%)]\t training loss: 19.693766\n",
      "epoch: 2 [9510/9880 (96%)]\t training loss: 19.812407\n",
      "epoch: 2 [9540/9880 (97%)]\t training loss: 12.768031\n",
      "epoch: 2 [9570/9880 (97%)]\t training loss: 9.246211\n",
      "epoch: 2 [9600/9880 (97%)]\t training loss: 4.356149\n",
      "epoch: 2 [9630/9880 (97%)]\t training loss: 19.441338\n",
      "epoch: 2 [9660/9880 (98%)]\t training loss: 12.465048\n",
      "epoch: 2 [9690/9880 (98%)]\t training loss: 11.056670\n",
      "epoch: 2 [9720/9880 (98%)]\t training loss: 26.428072\n",
      "epoch: 2 [9750/9880 (99%)]\t training loss: 12.379808\n",
      "epoch: 2 [9780/9880 (99%)]\t training loss: 10.822930\n",
      "epoch: 2 [9810/9880 (99%)]\t training loss: 5.422424\n",
      "epoch: 2 [9840/9880 (100%)]\t training loss: 10.530278\n",
      "epoch: 2 [9870/9880 (100%)]\t training loss: 10.272861\n",
      "\n",
      "Test dataset: Overall Loss: 4.2239,  (9880%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3): #For only two epochs (1 an 2) , training loop \n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
